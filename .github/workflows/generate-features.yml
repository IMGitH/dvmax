name: Generate Macro & Ticker Features

on:
  workflow_dispatch:
    inputs:
      restore_tag:
        description: "Release tag to restore (e.g., data-20250101_120000)"
        required: false
        type: string
      tickers_file:
        description: "Tickers file path (when mode=file)"
        required: false
        type: string
        default: config/us_tickers.txt
      start_year:
        description: "First year to generate (we use 12/31 of this year)"
        required: false
        type: string
        default: "2021"
      end_year:
        description: "Last year to generate (0 => current year)"
        required: false
        type: string
        default: "0"
      overwrite_mode:
        description: "append | overwrite | skip"
        required: false
        type: choice
        default: append
        options: [append, overwrite, skip]
      strict:
        description: "Fail run on hard errors (flagged rows never fail)"
        required: false
        type: boolean
        default: true
      force_merge:
        description: "Force re-merge of all parts at the end"
        required: false
        type: boolean
        default: false
      sleep_between_calls:
        description: "Seconds to sleep between tickers"
        required: false
        type: string
        default: "1.0"

permissions:
  contents: write

concurrency:
  group: features-${{ github.ref }}
  cancel-in-progress: false

jobs:
  generate:
    permissions:
      contents: read
      issues: write
    environment: dvmax-env
    runs-on: ubuntu-latest
    timeout-minutes: 120
    env:
      PYTHON_VERSION: "3.11"
      # Defaults; may be overridden by the "Configure env from inputs" step below
      OVERWRITE_MODE: "append"
      STRICT: "1"
      FORCE_MERGE: "0"
      START_YEAR: "2021"
      END_YEAR: "0"
      SLEEP_BETWEEN_CALLS: "1.0"
      TICKERS_FILE: "config/us_tickers_subset_limited.txt"

    steps:
      - name: ‚¨áÔ∏è Checkout
        uses: actions/checkout@v4

      - name: ‚öôÔ∏è Configure env from inputs
        run: |
          set -euo pipefail
          # map basic knobs
          echo "OVERWRITE_MODE=${{ inputs.overwrite_mode || 'append' }}" >> "$GITHUB_ENV"
          echo "STRICT=$([ '${{ inputs.strict }}' = 'true' ] && echo 1 || echo 0)" >> "$GITHUB_ENV"
          echo "FORCE_MERGE=$([ '${{ inputs.force_merge }}' = 'true' ] && echo 1 || echo 0)" >> "$GITHUB_ENV"
          echo "START_YEAR=${{ inputs.start_year || '2021' }}" >> "$GITHUB_ENV"
          echo "END_YEAR=${{ inputs.end_year || '0' }}" >> "$GITHUB_ENV"
          echo "SLEEP_BETWEEN_CALLS=${{ inputs.sleep_between_calls || '1.0' }}" >> "$GITHUB_ENV"
          echo "TICKERS=" >> "$GITHUB_ENV"
          echo "TICKERS_FILE=${{ inputs.tickers_file || 'config/us_tickers_subset_limited.txt' }}" >> "$GITHUB_ENV"

      - name: üìÅ Prepare isolated data root
        env:
          DATA_ROOT: ${{ runner.temp }}/features_data
        run: |
          set -e
          mkdir -p "${DATA_ROOT}"
          # Keep existing paths working by symlinking:
          rm -rf features_data 2>/dev/null || true
          ln -s "${DATA_ROOT}" features_data
          echo "DATA_ROOT=${DATA_ROOT}" >> $GITHUB_ENV

      - name: üß™ Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üóÑÔ∏è Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-
            pip-

      - name: üì¶ Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ‚ôªÔ∏è Restore snapshot strictly from the latest Release
      - name: ‚ôªÔ∏è Restore snapshot (Release only)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          DATA_ROOT="${DATA_ROOT:-${RUNNER_TEMP}/features_data}"
          mkdir -p "$DATA_ROOT"
          dl="${RUNNER_TEMP}/release_dl"
          mkdir -p "$dl"

          manual="${{ inputs.restore_tag || '' }}"
          if [ -n "$manual" ]; then
            tag="$manual"
          else
            # Find most recent published, non-prerelease release with a "data-" tag
            tag=$(gh release list --limit 30 --json tagName,isDraft,isPrerelease,publishedAt \
              --jq '[.[] | select(.isDraft==false and .isPrerelease==false and (.tagName|startswith("data-")))] 
                    | sort_by(.publishedAt) 
                    | last.tagName' || true)
          fi

          if [ -z "${tag:-}" ] || [ "${tag}" = "null" ]; then
            echo "No suitable Release found; starting fresh."
            exit 0
          fi

          echo "Using Release: $tag"
          gh release download "$tag" -p 'features_*.tar.gz' -D "$dl"

          tgz=$(ls -1 "$dl"/features_*.tar.gz 2>/dev/null | head -n1 || true)
          if [ -z "${tgz:-}" ]; then
            echo "No features_*.tar.gz asset in $tag; starting fresh."
            exit 0
          fi

          echo "Downloaded: $tgz"
          echo "sha256: $(sha256sum "$tgz" | cut -d' ' -f1)"

          # Extract directly into DATA_ROOT (tarball contains top-level dirs)
          tar -xzf "$tgz" -C "$DATA_ROOT"

          echo "Restored tree:"
          ls -R "$DATA_ROOT" || true

      # üîπ Run macro ALWAYS (it's not related to FMP)
      - name: üßÆ Run macro batch generation
        env:
          PYTHONPATH: .
        run: python src/dataprep/features/aggregation/macro_batch_runner.py

      # Now preflight only for the ticker part
      - name: üîé FMP preflight
        id: preflight
        env:
          PYTHONPATH: .
          FMP_API_KEY: ${{ secrets.FMP_API_KEY }}
        continue-on-error: true
        run: |
          set -eo pipefail
          code=0
          kind="OK"
          if python tools/fmp_preflight.py | tee preflight.log; then
            code=0; kind="OK"
          else
            code=${PIPESTATUS[0]:-1}
            if [ -f preflight.log ]; then
              kind=$(cut -d: -f1 preflight.log | head -n1 || echo "OTHER")
            else
              kind="OTHER"
            fi
          fi
          echo "code=$code" >> "$GITHUB_OUTPUT"
          echo "kind=$kind" >> "$GITHUB_OUTPUT"
          exit $code

      - name: ‚ùå Stop on hard FMP failures
        if: steps.preflight.outputs.code != '0' && (steps.preflight.outputs.kind == 'PLAN' || steps.preflight.outputs.kind == 'AUTH')
        run: |
          echo "Hard failure: ${{ steps.preflight.outputs.kind }} ‚Äî not retryable."
          exit 1

      # üîπ Ticker generation only when preflight is OK / retryable
      - name: üßÆ Run ticker batch generation
        if: steps.preflight.outputs.kind == 'OK' || steps.preflight.outputs.kind == 'RATE' || steps.preflight.outputs.kind == 'SERVER' || steps.preflight.outputs.kind == 'OTHER'
        env:
          PYTHONPATH: .
          FMP_API_KEY: ${{ secrets.FMP_API_KEY }}
          # pass through env configured earlier
          FORCE_MERGE: "${{ env.FORCE_MERGE }}"
          OVERWRITE_MODE: "${{ env.OVERWRITE_MODE }}"
          STRICT: "${{ env.STRICT }}"
          START_YEAR: "${{ env.START_YEAR }}"
          END_YEAR: "${{ env.END_YEAR }}"
          SLEEP_BETWEEN_CALLS: "${{ env.SLEEP_BETWEEN_CALLS }}"
          TICKERS_FILE: "${{ env.TICKERS_FILE }}"
          TICKERS: "${{ env.TICKERS }}"
        run: python src/dataprep/features/aggregation/ticker_batch_runner.py

      - name: üóÇÔ∏è Build processed tickers ledger
        if: always()
        run: |
          python - <<'PY'
          import json, glob
          from pathlib import Path
          done = { Path(p).stem: "ok" for p in glob.glob("features_data/tickers_history/*.parquet") }
          Path("features_data/status").mkdir(parents=True, exist_ok=True)
          Path("features_data/status/processed.json").write_text(json.dumps(done, indent=2))
          print("Wrote features_data/status/processed.json with", len(done), "tickers")
          PY

      - name: üß™ Show validation flags
        if: always()
        run: |
          echo "üîç Checking for soft validation flags..."
          if compgen -G "features_data/_audit/*.txt" > /dev/null; then
            echo "‚ö†Ô∏è Flagged rows:"
            for f in features_data/_audit/*.txt; do
              echo ""
              echo "----- $(basename "$f") -----"
              cat "$f"
            done
          else
            echo "‚úÖ No flags."
          fi

      - name: üõ†Ô∏è Install jq for JSON processing
        if: always()
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: üìã Append live progress to summary
        if: always()
        run: |
          if [ -f features_data/status/progress.json ]; then
            echo "## Data Progress" >> $GITHUB_STEP_SUMMARY
            echo "- Percent: $(jq -r .percent features_data/status/progress.json)%" >> $GITHUB_STEP_SUMMARY
            echo "- Processed: $(jq -r .counts.processed features_data/status/progress.json)/$(jq -r .totals.total_tasks features_data/status/progress.json)" >> $GITHUB_STEP_SUMMARY
            echo "- Flagged: $(jq -r .counts.flagged features_data/status/progress.json)" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: $(jq -r .counts.failed features_data/status/progress.json)" >> $GITHUB_STEP_SUMMARY
            echo "- ETA (UTC): $(jq -r .eta_utc features_data/status/progress.json)" >> $GITHUB_STEP_SUMMARY
          else
            echo "No progress.json found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: üì® Notify Progress via GitHub Issue
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = 'features_data/status/progress.json';
            if (!fs.existsSync(path)) {
              core.info('progress.json not found; skipping issue notification.');
              return;
            }
            const p = JSON.parse(fs.readFileSync(path,'utf8'));
            const title = `Data snapshot ready: ${p.percent}% processed`;
            const body = `**Data snapshot completed**
            - Percent: ${p.percent}%
            - Processed: ${p.counts.processed}/${p.totals.total_tasks}
            - Flagged: ${p.counts.flagged}
            - Failed: ${p.counts.failed}
            - ETA (UTC): ${p.eta_utc}

            cc @your-github-handle`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title, body
            });

      - name: üéØ Threshold milestones
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            if (!fs.existsSync('features_data/status/progress.json')) return;
            const p = JSON.parse(fs.readFileSync('features_data/status/progress.json','utf8'));
            const percent = Math.floor(p.percent || 0);
            const hit = [50, 90].includes(percent);
            if (!hit) return;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Milestone ${percent}% reached`,
              body: `Processed ${p.counts.processed}/${p.totals.total_tasks} | Failed ${p.counts.failed} | ETA ${p.eta_utc || 'n/a'}`
            });

      - name: üìé Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs
          path: |
            preflight.log
            **/*.log
            features_data/status/*.json
            features_data/_audit/*.txt
          if-no-files-found: ignore
          retention-days: 7

      - name: üíæ Upload partial results on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: features_partial
          path: |
            features_data/macro_history
            features_data/tickers_history
            features_data/tickers_static
            features_data/status
            features_data/_audit
          if-no-files-found: ignore
          retention-days: 7

      # üì¶ Package outputs from DATA_ROOT, not repo
      - name: üì¶ Package outputs
        id: package
        if: success()
        run: |
          set -e
          ts=$(date -u +'%Y%m%d_%H%M%S')
          echo "TS=$ts" >> $GITHUB_OUTPUT
          # Build a clean payload under $RUNNER_TEMP and tar it
          pkg_dir="${RUNNER_TEMP}/pkg_${ts}"
          mkdir -p "$pkg_dir"
          for d in macro_history tickers_history tickers_static status _audit; do
            if [ -d "${DATA_ROOT}/${d}" ]; then
              mkdir -p "${pkg_dir}/${d}"
              cp -R "${DATA_ROOT}/${d}/." "${pkg_dir}/${d}/"
            fi
          done
          tar -czf "features_${ts}.tar.gz" -C "${pkg_dir}" .
          ls -lh "features_${ts}.tar.gz"

      # Only publish releases from this repo & trusted branches (adjust ORG/REPO and branch)
      - name: üè∑Ô∏è Create release & upload asset
        if: success() && github.repository == 'ORG/REPO' && startsWith(github.ref, 'refs/heads/main')
        uses: softprops/action-gh-release@v2
        with:
          token: ${{ secrets.RELEASE_TOKEN }}   # ‚úÖ PAT with repo:write
          tag_name: data-${{ steps.package.outputs.TS }}
          name: "Data snapshot ${{ steps.package.outputs.TS }}"
          body: |
            Automated data snapshot.
            - Timestamp (UTC): ${{ steps.package.outputs.TS }}
            - OVERWRITE_MODE: ${{ env.OVERWRITE_MODE }}
          target_commitish: ${{ github.sha }}
          files: features_${{ steps.package.outputs.TS }}.tar.gz
