{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0477a01d",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Integrated Dividend Feature Analysis\n",
    "\n",
    "This notebook combines visual correlation analysis, regression diagnostics, and model-based feature importance to analyze the impact of various features on dividend-related metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & robust loading ---\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load all ticker time-series\n",
    "history_files = glob.glob(\"features_data/tickers_history/*.parquet\")\n",
    "if not history_files:\n",
    "    raise FileNotFoundError(\"No parquet files under features_data/tickers_history/\")\n",
    "df_features = pd.concat([pd.read_parquet(f) for f in history_files], ignore_index=True)\n",
    "\n",
    "# 2) Load static raw + OHE (handle older files without raw labels)\n",
    "static_path = \"features_data/tickers_static/static_ticker_info.parquet\"\n",
    "if not os.path.exists(static_path):\n",
    "    raise FileNotFoundError(static_path)\n",
    "df_static_raw = pd.read_parquet(static_path)\n",
    "\n",
    "ohe_path = \"features_data/tickers_static/static_ohe.parquet\"\n",
    "if os.path.exists(ohe_path):\n",
    "    df_static_ohe = pd.read_parquet(ohe_path)\n",
    "else:\n",
    "    # fallback: pick OHE cols from raw file\n",
    "    df_static_ohe = df_static_raw.filter(regex=r\"^ticker$|^(sector_|country_)\")\n",
    "\n",
    "def _infer_label_from_ohe(df, prefix: str) -> pd.Series:\n",
    "    \"\"\"Infer human label from OHE columns; returns NaN if none present.\"\"\"\n",
    "    ohe_cols = [c for c in df.columns if c.startswith(prefix + \"_\") and c != f\"{prefix}_other\"]\n",
    "    if not ohe_cols:\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    idx = df[ohe_cols].idxmax(axis=1)\n",
    "    lab = (\n",
    "        idx.str.replace(f\"{prefix}_\", \"\", regex=False)\n",
    "           .str.replace(\"_\", \" \")\n",
    "           .str.title()\n",
    "           .replace({\"Usa\": \"USA\", \"Uk\": \"UK\", \"Uae\": \"UAE\"})\n",
    "    )\n",
    "    return lab\n",
    "\n",
    "# Build raw labels (ticker + country/sector if present; else infer from OHE)\n",
    "raw_cols = [c for c in [\"ticker\", \"country\", \"sector\"] if c in df_static_raw.columns]\n",
    "raw_labels = df_static_raw[raw_cols].copy() if raw_cols else df_static_ohe[[\"ticker\"]].copy()\n",
    "\n",
    "if \"sector\" not in raw_labels.columns:\n",
    "    raw_labels[\"sector\"] = _infer_label_from_ohe(df_static_ohe, \"sector\")\n",
    "if \"country\" not in raw_labels.columns:\n",
    "    raw_labels[\"country\"] = _infer_label_from_ohe(df_static_ohe, \"country\")\n",
    "\n",
    "# Merge labels + OHE; only select columns that exist\n",
    "ohe_cols = [c for c in df_static_ohe.columns if c != \"ticker\"]\n",
    "df_static = raw_labels.merge(df_static_ohe, on=\"ticker\", how=\"left\")\n",
    "keep_cols = [\"ticker\"] + [c for c in [\"country\", \"sector\"] if c in df_static.columns] + ohe_cols\n",
    "\n",
    "df_features = df_features.merge(df_static[keep_cols], on=\"ticker\", how=\"left\")\n",
    "\n",
    "# 3) Normalize countries for macro join\n",
    "COUNTRY_NORMALIZATION = {\n",
    "    \"USA\": \"United States\",\n",
    "    \"UK\": \"United Kingdom\",\n",
    "    \"UAE\": \"United Arab Emirates\",\n",
    "    \"Korea\": \"South Korea\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Czechia\": \"Czech Republic\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Hong Kong\": \"Hong Kong SAR\",\n",
    "}\n",
    "if \"country\" not in df_features.columns:\n",
    "    df_features[\"country\"] = np.nan\n",
    "df_features[\"country_norm\"] = df_features[\"country\"].map(COUNTRY_NORMALIZATION).fillna(df_features[\"country\"])\n",
    "\n",
    "# 4) Load ALL macro files and concat\n",
    "macro_files = glob.glob(\"features_data/macro_history/*.parquet\")\n",
    "if not macro_files:\n",
    "    print(\"âš ï¸ No macro files found under features_data/macro_history/. Macro merge will be empty.\")\n",
    "    df_macro_all = pd.DataFrame(columns=[\"as_of_year\", \"country\"])\n",
    "else:\n",
    "    df_macro_all = pd.concat([pd.read_parquet(f) for f in macro_files], ignore_index=True)\n",
    "\n",
    "# 5) Year alignment\n",
    "df_features[\"as_of\"] = pd.to_datetime(df_features[\"as_of\"])\n",
    "df_features[\"as_of_year\"] = df_features[\"as_of\"].dt.year.astype(\"Int64\")\n",
    "if \"as_of_year\" in df_macro_all.columns and not pd.api.types.is_integer_dtype(df_macro_all[\"as_of_year\"]):\n",
    "    df_macro_all[\"as_of_year\"] = df_macro_all[\"as_of_year\"].astype(\"Int64\")\n",
    "\n",
    "# 6) Merge features+static with macro on (year, normalized country)\n",
    "df_merged = df_features.merge(\n",
    "    df_macro_all,\n",
    "    left_on=[\"as_of_year\", \"country_norm\"],\n",
    "    right_on=[\"as_of_year\", \"country\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_macro\"),\n",
    ")\n",
    "\n",
    "# Ensure OHEs are numeric 0/1 floats\n",
    "for col in [c for c in df_merged.columns if c.startswith(\"sector_\") or c.startswith(\"country_\")]:\n",
    "    df_merged[col] = pd.to_numeric(df_merged[col], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
    "\n",
    "# Cleanups & quick checks\n",
    "for col in (\"backfilled_year\",):\n",
    "    if col in df_merged.columns:\n",
    "        df_merged.drop(columns=[col], inplace=True)\n",
    "\n",
    "print(\"Data loaded and merged successfully.\")\n",
    "print(\"df_features country (raw) unique:\", sorted(df_features[\"country\"].dropna().unique().tolist()))\n",
    "print(\"df_features country_norm unique:\", sorted(df_features[\"country_norm\"].dropna().unique().tolist()))\n",
    "print(\"macro countries:\", sorted(df_macro_all[\"country\"].dropna().unique().tolist()) if \"country\" in df_macro_all.columns else \"N/A\")\n",
    "print(\"Merge macro coverage (non-null gdp_yoy_backfilled):\", df_merged.get(\"gdp_yoy_backfilled\").notna().mean() if \"gdp_yoy_backfilled\" in df_merged.columns else 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaaf3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_features countries:\", df_features['country'].unique())\n",
    "print(\"df_macro countries:\", df_macro['country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd69943",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVIDEND_FEATURES = [\n",
    "    \"dividend_yield\",\n",
    "    \"dividend_cagr_3y\",\n",
    "    \"dividend_cagr_5y\",\n",
    "    \"yield_vs_5y_median\",\n",
    "]\n",
    "\n",
    "SECTOR_COLS = sorted([c for c in df_merged.columns if c.startswith(\"sector_\") and c != \"sector_relative_6m\"])\n",
    "COUNTRY_COLS = sorted([c for c in df_merged.columns if c.startswith(\"country_\")])\n",
    "HAS_COLS = sorted([c for c in df_merged.columns if c.startswith(\"has_\")])\n",
    "\n",
    "BINARY_FEATURES = SECTOR_COLS + COUNTRY_COLS + HAS_COLS\n",
    "EXCLUDE_FEATURES = [\"ticker\", \"as_of\", \"as_of_year\", \"country\", \"country_norm\", \"sector\"] + DIVIDEND_FEATURES\n",
    "\n",
    "all_columns = df_merged.columns.tolist()\n",
    "independent_features = [col for col in all_columns if col not in EXCLUDE_FEATURES]\n",
    "numerical_independent_features = [f for f in independent_features if f not in BINARY_FEATURES]\n",
    "categorical_independent_features = [f for f in independent_features if f in BINARY_FEATURES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db57f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_categories = {\n",
    "    \"Financial_Metrics\": [\n",
    "        f for f in numerical_independent_features\n",
    "        if any(k in f for k in (\"return\", \"ratio\", \"cagr\", \"cover\", \"volatility\", \"drawdown\"))\n",
    "    ],\n",
    "    \"Macro_Economic\": [\n",
    "        col for col in [\"gdp_yoy_backfilled\", \"inflation_latest\", \"unemployment_latest\", \"consumption_backfilled\"]\n",
    "        if col in df_merged.columns\n",
    "    ],\n",
    "    \"Sector_OHE\": SECTOR_COLS,\n",
    "    \"Country_OHE\": COUNTRY_COLS,\n",
    "}\n",
    "predictor_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ca30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c722fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_regression(df, target_feature, numerical_features):\n",
    "    for feature in numerical_features:\n",
    "        if (\n",
    "            feature in df.columns\n",
    "            and df[feature].notna().sum() >= 3\n",
    "            and df[feature].nunique() > 2\n",
    "            and not feature.endswith('_capped')\n",
    "            and not feature.startswith('has_')\n",
    "        ):\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            sns.scatterplot(x=feature, y=target_feature, data=df, alpha=0.6)\n",
    "            sns.regplot(x=feature, y=target_feature, data=df, scatter=False, color='red', line_kws={'alpha': 0.6})\n",
    "            plt.title(f'{target_feature} vs. {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def plot_sector_boxplot(df, target_feature):\n",
    "    sector_cols = [f for f in df.columns if f.startswith(\"sector_\") and f != \"sector_relative_6m\"]\n",
    "    if not sector_cols:\n",
    "        return df\n",
    "    tmp = df.copy()\n",
    "    tmp[\"sector_label\"] = (\n",
    "        tmp[sector_cols].idxmax(axis=1)\n",
    "            .str.replace(\"^sector_\", \"\", regex=True)\n",
    "            .str.replace(\"_\", \" \").str.title()\n",
    "    )\n",
    "    if tmp[\"sector_label\"].nunique() > 1 and tmp[target_feature].notna().sum() >= 5:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(x=\"sector_label\", y=target_feature, data=tmp)\n",
    "        plt.title(f\"{target_feature} distribution by sector\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return df\n",
    "\n",
    "def plot_binary_categorical_boxplots(df, target_feature, categorical_features):\n",
    "    for feature in categorical_features:\n",
    "        if (\n",
    "            feature in df.columns\n",
    "            and not feature.startswith('has_')\n",
    "            and not feature.endswith('_capped')\n",
    "        ):\n",
    "            df[feature] = df[feature].astype('category')\n",
    "            if df[feature].nunique() <= 1:\n",
    "                continue\n",
    "            value_counts = df[feature].value_counts(normalize=True)\n",
    "            if value_counts.iloc[0] > 0.95:\n",
    "                continue\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            sns.boxplot(x=feature, y=target_feature, data=df)\n",
    "            plt.title(f'{target_feature} distribution by {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def plot_correlations(df, target_feature, predictor_categories, max_n_plots):\n",
    "    fig, axes = plt.subplots(1, len(predictor_categories), figsize=(5 * len(predictor_categories), 4))\n",
    "    if len(predictor_categories) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (group_name, group_features) in zip(axes, predictor_categories.items()):\n",
    "        valid_feats = [\n",
    "            f for f in group_features if (\n",
    "                f in df.columns and \n",
    "                df[f].notna().sum() > 2 and \n",
    "                not f.startswith('has_') and \n",
    "                not f.endswith('_capped') and \n",
    "                not f.startswith('sector_')\n",
    "            )\n",
    "        ]\n",
    "        if not valid_feats:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        data_for_corr = df[valid_feats + [target_feature]].dropna()\n",
    "        if data_for_corr.shape[0] < 3:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        corr_vals = data_for_corr.corr()[target_feature].drop(target_feature).dropna()\n",
    "        corr_vals = corr_vals.sort_values(key=np.abs, ascending=False).head(max_n_plots)\n",
    "        colors = ['red' if v < 0 else 'blue' for v in corr_vals]\n",
    "        ax.barh(corr_vals.index.str.replace('_', ' ').str.title(), corr_vals.values, color=colors)\n",
    "        ax.set_title(f\"{group_name} Correlation\")\n",
    "        ax.axvline(0, color='black', lw=0.8)\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_random_forest_importance(df, target_feature, predictor_categories, max_n_importance):\n",
    "    rf_features = [\n",
    "        f for group in predictor_categories.values() for f in group\n",
    "        if f in df.columns and not f.endswith('_capped') and not f.startswith('has_')\n",
    "    ]\n",
    "    model_data = df[rf_features + [target_feature]].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if len(model_data) < 10 or model_data.shape[1] < 2:\n",
    "        print(\"Not enough data for modeling.\")\n",
    "        return\n",
    "    X = model_data[rf_features]\n",
    "    y = model_data[target_feature]\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X, y)\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\"Feature\": X.columns, \"Importance\": rf_model.feature_importances_})\n",
    "          .sort_values(\"Importance\", ascending=False).head(max_n_importance)\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(8, 0.3 * len(importance_df)))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))\n",
    "    bars = ax.barh(range(len(importance_df)), importance_df[\"Importance\"], color=colors)\n",
    "    ax.set_yticks(range(len(importance_df)))\n",
    "    ax.set_yticklabels([f.replace('_', ' ').title() for f in importance_df[\"Feature\"]])\n",
    "    ax.set_xlabel('Feature Importance (Random Forest)')\n",
    "    ax.set_title(f'Feature Importance for Predicting {target_feature.replace(\"_\", \" \").title()}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for bar, importance in zip(bars, importance_df[\"Importance\"]):\n",
    "        ax.text(importance + 0.001, bar.get_y() + bar.get_height() / 2, f'{importance:.3f}', ha='left', va='center', fontsize=8)\n",
    "    plt.tight_layout(); plt.show()\n",
    "    print(f\"Model RÂ² Score: {r2_score(y, rf_model.predict(X)):.3f}\")\n",
    "    return importance_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_impact(target_feature):\n",
    "    df_analysis = df_merged[df_merged[target_feature].notna()].copy()\n",
    "    if df_analysis.empty or df_analysis[target_feature].nunique() <= 1:\n",
    "        print(f\"âš ï¸ '{target_feature}' has insufficient variation. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ” Analyzing: {target_feature}\")\n",
    "    plot_scatter_regression(df_analysis, target_feature, numerical_independent_features)\n",
    "    df_analysis = plot_sector_boxplot(df_analysis, target_feature)\n",
    "    plot_binary_categorical_boxplots(df_analysis, target_feature, categorical_independent_features)\n",
    "    plot_correlations(df_analysis, target_feature, predictor_categories, max_n_plots=10)\n",
    "    plot_random_forest_importance(df_analysis, target_feature, predictor_categories, max_n_importance=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13afc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in DIVIDEND_FEATURES:\n",
    "    print (f\"\\n\\n{'=' * 40}\\nAnalyzing feature: {target}\\n{'=' * 40}\")\n",
    "    analyze_feature_impact(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for target in DIVIDEND_FEATURES:\n",
    "#     analyze_feature_impact(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59c563",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Partial Correlation Heatmap using valid features only\n",
    "ðŸ” What this does:\n",
    "- Computes pairwise correlations.\n",
    "- Selects only features with at least one absolute correlation â‰¥ 0.3 to a dividend feature.\n",
    "- Shows a cleaner, interpretable heatmap using clustermap (which groups similar variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_features = DIVIDEND_FEATURES + numerical_independent_features\n",
    "valid_corr_features = [\n",
    "    col for col in correlation_features\n",
    "    if col in df_merged.columns and df_merged[col].dtype in ['float64', 'float32', 'int64']\n",
    "]\n",
    "\n",
    "# Keep only columns with â‰¥5 non-NaN values\n",
    "valid_counts = df_merged[valid_corr_features].notna().sum()\n",
    "selected_features = valid_counts[valid_counts >= 5].index.tolist()\n",
    "\n",
    "# Configuration\n",
    "target_features = DIVIDEND_FEATURES\n",
    "threshold = 0.3  # Adjust this threshold as needed\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_merged[selected_features].corr()\n",
    "\n",
    "# Identify features with at least one strong correlation with any dividend feature\n",
    "strong_corrs = corr_matrix[target_features].abs().max(axis=1)\n",
    "filtered_features = strong_corrs[strong_corrs >= threshold].index.tolist()\n",
    "\n",
    "# If nothing is found, fallback to a warning\n",
    "if len(filtered_features) < 2:\n",
    "    print(f\"âš ï¸ No features found with |correlation| â‰¥ {threshold} to any dividend feature.\")\n",
    "else:\n",
    "    # Compute filtered correlation matrix\n",
    "    filtered_corr = df_merged[filtered_features].corr()\n",
    "\n",
    "    # Optional: cluster to improve interpretability\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        sns.clustermap(\n",
    "            filtered_corr,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            linewidths=0.5,\n",
    "            figsize=(12, 10)\n",
    "        )\n",
    "        plt.title(f\"Filtered Correlation Heatmap (|r| â‰¥ {threshold})\", pad=80)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to render heatmap: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f7568",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Top Insights on Dividend Features**\n",
    "\n",
    "#### 1. **Dividend Yield**\n",
    "\n",
    "* Strong **positive correlation** with:\n",
    "\n",
    "  * `payout_ratio` (r â‰ˆ 0.73)\n",
    "  * `net_debt_to_ebitda` (r â‰ˆ 0.57)\n",
    "* Strong **negative correlation** with:\n",
    "\n",
    "  * `sma_50_200_delta`, `fcf_cagr_3y`, and `volatility` (r â‰ˆ -0.5 to -0.6)\n",
    "\n",
    "ðŸ§  *Interpretation*: High dividend yield tends to occur in **financially mature, lower-growth stocks** with higher debt and slower momentum.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Dividend CAGR (3Y, 5Y)**\n",
    "\n",
    "* Strong **positive correlation** with:\n",
    "\n",
    "  * `fcf_cagr_3y`, `eps_cagr_3y` (r â‰ˆ 0.8+)\n",
    "  * `sma_50_200_delta`, `6m_return`, `12m_return` (r â‰ˆ 0.5â€“0.8)\n",
    "\n",
    "ðŸ§  *Interpretation*: Sustained dividend growth is **highly aligned with earnings and cash flow growth**, and coincides with **positive price momentum**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Yield vs. 5Y Median**\n",
    "\n",
    "* Strong **negative correlation** with:\n",
    "\n",
    "  * `fcf_cagr_3y`, `dividend_cagr_5y`, `sma_50_200_delta` (r â‰ˆ -0.5 to -0.8)\n",
    "\n",
    "ðŸ§  *Interpretation*: When dividend growth and performance are high, yields tend to be **compressed vs. historical median**, likely due to price appreciation.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¥ Additional Highlights\n",
    "\n",
    "* `fcf_cagr_3y` and `eps_cagr_3y` are **core growth drivers**, highly correlated with most dividend-growth indicators.\n",
    "* `sector_relative_6m` and `sma_50_200_delta` are also informative of dividend trends, linking **technical momentum** to dividend behaviors.\n",
    "* `net_debt_to_ebitda` shows up as a red flag for **high yield but low growth**, supporting a â€œvalue trapâ€ narrative in some cases.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”š Summary\n",
    "\n",
    "* **Yield**: linked to **debt, payout, and negative momentum**.\n",
    "* **Growth**: driven by **fundamentals (FCF, EPS)** and **positive technicals**.\n",
    "* **Over-yielding**: may signal **underperformance or risk**, not opportunity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
